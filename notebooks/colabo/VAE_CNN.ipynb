{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN VAE for SRL for DIY Self driving car\n",
    "\n",
    "In this notebook you will learn the CNN VAE(beta) model. The result model is used for state representation in reinforcement learning.\n",
    "\n",
    "First collection training data. you can use notebooks\\utility\\data_collection.ipynb \n",
    "\n",
    "Collect images of the course while driving the car on the course. Collect 1k to 10k images. Adjust the number of data collected according to the size of the course. When running the course, run in the center of the course, the side of the side line, zigzag running, etc. During the trial during reinforcement learning, you do not know how to run on the course. Collect data so that the course can be represented in the event of an error.\n",
    "\n",
    "\n",
    "## Mount google drive\n",
    "\n",
    "You upload zip file that contain training data. The zip file copy from googledrive. \n",
    "Set zip file name to DATASET_ZIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b8q352pxlqy",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATASET_ZIP = 'dataset.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy from google drive\n",
    "\n",
    "Copy training data and unzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ofIwQJ-J-qLM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "!cp '/content/drive/My Drive/$DATASET_ZIP' ./\n",
    "!unzip -q DATASET_ZIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "nKd7ixEGblO-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import glob\n",
    "\n",
    "files = glob.glob(os.path.join('/content', 'DATASET_ZIP', '*.jpg')\n",
    "for f in files:\n",
    "  try:\n",
    "    image = PIL.Image.open(f)\n",
    "  except OSError:\n",
    "    print('Delete' + f)\n",
    "    !rm -rf f\n",
    "  image = image.resize((160,120))\n",
    "  image.crop((0, 40, 160, 120)).save(f, quality=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jKObTxXVxUyT",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "x1w9dHMDxc2t",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "f8kgmDhAyVe8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "dataset = datasets.ImageFolder(root='./roll', transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "len(dataset.imgs), len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qxYHQhNKyX0q",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'real_image.png')\n",
    "Image('real_image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVLGbLF8yhy7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tyFNicuqyj6s",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=256):\n",
    "        return input.view(input.size(0), size, 3, 8)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=6144, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.randn(*mu.size()).to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), F.softplus(self.fc2(h))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def loss_fn(self, images, reconst, mean, logvar):\n",
    "        KL = -0.5 * torch.sum((1 + logvar - mean.pow(2) - logvar.exp()), dim=0)\n",
    "        KL = torch.mean(KL)\n",
    "        reconstruction = F.binary_cross_entropy(reconst.view(-1,38400), images.view(-1, 38400), reduction='sum') #size_average=False)\n",
    "        return reconstruction + 5.0 * KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2wTqO-JyncH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Prepare Training\n",
    "\n",
    "Create VAE model and initialize optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zDELfI7MUp2C",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "VARIANTS_SIZE = 32\n",
    "image_channels = fixed_x.size(1)\n",
    "vae = VAE(image_channels=image_channels, z_dim=VARIANTS_SIZE ).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "summary(vae, (3, 80, 160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n73BXe26UsiP",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Hmvqsf_fw5Sp",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztKsDNhTU20H",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## conmple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVllkoOoU5c4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "vae.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    grid = None\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss = vae.loss_fn(images, recon_images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        grid = torchvision.utils.make_grid(recon_images)\n",
    "    writer.add_image('Image/reconst', grid, epoch)\n",
    "    writer.add_scalar('Loss/train',np.average(losses), epoch)\n",
    "    print(\"EPOCH: {} loss: {}\".format(epoch+1, np.average(losses)))\n",
    "\n",
    "torch.save(vae.state_dict(), 'vae.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup\n",
    "\n",
    "Copy trained model file to GoogleDrive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "DFSRPBnAsG2g",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!cp vae.torch '/content/drive/My Drive/vae.torch'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "VAE-CNN.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
